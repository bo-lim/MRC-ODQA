TrainingArguments:
    output_dir: './prediction/'
    overwrite_output_dir: True
    save_total_limit: 5
    save_steps: 200
    seed: 42

    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8

    logging_dir: './logs'
    logging_steps: 100

    evaluation_strategy: 'steps'
    eval_steps: 500
    load_best_model_at_end: True

    do_eval: False
    do_predict: True

    fp16: True
    fp16_opt_level: 'O1'

    label_smoothing_factor: 0.0

    report_to: 'wandb'


ModelArguments:
    model_name_or_path: './output/model'
    config_name: 'klue/roberta-large'
    tokenizer_name: 'klue/roberta-large'


DataTrainingArguments:
    dataset_name: '../data/test_dataset'
    overwrite_cache: False
    preprocessing_num_workers: null

    max_seq_length: 384
    pad_to_max_length: False
    doc_stride: 128
    max_answer_length: 30
    
    eval_retrieval: True
    num_clusters: 64
    top_k_retrieval: 10
    use_faiss: False