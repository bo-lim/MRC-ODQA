TrainingArguments:
    output_dir: './output0511-4/'
    save_total_limit: 5
    save_steps: 500
    seed: 42

    num_train_epochs: 4
    learning_rate: 0.00002
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16

    weight_decay: 0
    lr_scheduler_type: 'linear'
    gradient_accumulation_steps: 1
    
    warmup_ratio: 0.0
    warmup_steps: 0

    logging_dir: './logs'
    logging_steps: 100

    evaluation_strategy: 'steps'
    eval_steps: 500
    #load_best_model_at_end: True

    do_train: True
    do_eval: True

    fp16: True
    fp16_opt_level: 'O1'

    label_smoothing_factor: 0.0

    report_to: 'wandb'
    run_name: 'bolim/top_k 20-rogL'


ModelArguments:
    model_name_or_path: 'klue/roberta-large'
    config_name: null
    tokenizer_name: null


DataTrainingArguments:
    dataset_name: '../data/train_dataset'
    overwrite_cache: True
    preprocessing_num_workers: null

    max_seq_length: 384
    pad_to_max_length: False
    doc_stride: 128
    max_answer_length: 30
    
    eval_retrieval: True
    num_clusters: 64
    top_k_retrieval: 20
    use_faiss: False