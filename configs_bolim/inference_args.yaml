TrainingArguments:
<<<<<<< HEAD:configs/inference_args.yaml
    output_dir: './prediction/'
    overwrite_output_dir: True
=======
    output_dir: './predictionBM25+/'
>>>>>>> a8b0f038233d3bfbd706db66d04c89ce8cd7eeed:configs_bolim/inference_args.yaml
    save_total_limit: 5
    save_steps: 200
    seed: 42

    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8

    logging_dir: './logs'
    logging_steps: 100

    evaluation_strategy: 'steps'
    eval_steps: 500
    load_best_model_at_end: True

    do_eval: False
    do_predict: True

    fp16: True
    fp16_opt_level: 'O1'

    label_smoothing_factor: 0.0

    report_to: 'wandb'


ModelArguments:
<<<<<<< HEAD:configs/inference_args.yaml
    model_name_or_path: './output/model'
    config_name: 'klue/roberta-large'
    tokenizer_name: 'klue/roberta-large'
=======
    model_name_or_path: './output(20)'
    config_name: null
    tokenizer_name: null
>>>>>>> a8b0f038233d3bfbd706db66d04c89ce8cd7eeed:configs_bolim/inference_args.yaml


DataTrainingArguments:
    dataset_name: '../data/test_dataset'
<<<<<<< HEAD:configs/inference_args.yaml
    overwrite_cache: False
=======
    overwrite_cache: True
>>>>>>> a8b0f038233d3bfbd706db66d04c89ce8cd7eeed:configs_bolim/inference_args.yaml
    preprocessing_num_workers: null

    max_seq_length: 384
    pad_to_max_length: False
    doc_stride: 128
    max_answer_length: 30
    
    eval_retrieval: True
    num_clusters: 64
    top_k_retrieval: 20
    use_faiss: False

#    bm25 : True